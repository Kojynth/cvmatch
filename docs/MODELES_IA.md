# Documentation des Mod√®les IA - CVMatch

## üìä √âvaluation de votre configuration




---

## ü§ñ Mod√®les IA Support√©s

### Mod√®les GPU (CUDA requis)

#### üèÜ **Qwen2.5-32B** (Premium)
- **HuggingFace :** https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
- **VRAM :** 24GB+ requis
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)
- **Vitesse :** ‚ö° (1/3)
- **Usage :** Candidatures critiques, qualit√© ultime
- **Quantification :** AWQ

#### ü•à **Qwen2.5-14B** (√âquilibr√©)
- **HuggingFace :** https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
- **VRAM :** 8GB+ requis
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)
- **Vitesse :** ‚ö°‚ö° (2/3)
- **Usage :** CV professionnels, excellente qualit√©
- **Quantification :** GPTQ

#### ü•â **Qwen2.5-7B** (Rapide)
- **HuggingFace :** https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- **VRAM :** 4GB+ requis
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
- **Vitesse :** ‚ö°‚ö° (2/3)
- **Usage :** Excellent √©quilibre qualit√©/vitesse
- **Quantification :** GPTQ
- **üéÆ Compatible RTX 4050**

#### **Mistral-7B** (L√©ger)
- **HuggingFace :** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **VRAM :** 4GB+ requis
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê (3/5)
- **Vitesse :** ‚ö°‚ö°‚ö° (3/3)
- **Usage :** G√©n√©ration CV standard, rapide
- **Quantification :** GPTQ
- **üéÆ Compatible RTX 4050**

### Mod√®les CPU (Sans GPU)

#### üèÜ **Phi-3-Mini** (CPU Premium)
- **HuggingFace :** https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
- **GitHub :** https://github.com/microsoft/Phi-3
- **RAM :** Optimis√© pour toute config
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
- **Vitesse :** ‚ö°‚ö°‚ö° (3/3)
- **Usage :** Mod√®le Microsoft optimis√© CPU
- **Quantification :** INT8

#### **Qwen2.5-1.5B** (CPU √âquilibr√©)
- **HuggingFace :** https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
- **RAM :** 8GB+ recommand√©
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)
- **Vitesse :** ‚ö°‚ö° (2/3)
- **Usage :** Version l√©g√®re de Qwen
- **Quantification :** INT8

#### **TinyLlama** (CPU Ultra-rapide)
- **HuggingFace :** https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0
- **GitHub :** https://github.com/jzhang38/TinyLlama
- **RAM :** 4GB+ suffisant
- **Qualit√© :** ‚≠ê‚≠ê‚≠ê (3/5)
- **Vitesse :** ‚ö°‚ö°‚ö° (3/3)
- **Usage :** Mod√®le 1B ultra-l√©ger
- **Quantification :** INT8

---

## üéØ Recommandations pour votreConfiguration 

### Configuration Optimale
1. **Mod√®le recommand√© :** Qwen2.5-7B ou Mistral-7B
2. **Quantification :** GPTQ 4-bit ou AWQ
3. **Moteur :** vLLM avec optimisations RTX 4050
4. **GPU Memory Utilization :** 85% max

### Optimisations Sp√©ciales RTX 4050
- ‚úÖ GGML Q4 quantization (recommand√© pour 32B)
- ‚úÖ AWQ quantization (√©quilibre vitesse/qualit√©)
- ‚úÖ FlashAttention-2 si disponible
- ‚úÖ Cache CUDA vid√© r√©guli√®rement
- ‚úÖ Surveillance temp√©rature GPU

---

## üîß Technologies d'Optimisation

### Quantification
- **GPTQ :** https://github.com/IST-DASLab/gptq
- **AWQ :** https://github.com/mit-han-lab/llm-awq
- **GGML :** https://github.com/ggerganov/ggml

### Moteurs d'Inf√©rence
- **vLLM :** https://github.com/vllm-project/vllm
- **ExLlamaV2 :** https://github.com/turboderp/exllamav2
- **ctranslate2 :** https://github.com/OpenNMT/CTranslate2

### Optimisations
- **FlashAttention :** https://github.com/Dao-AILab/flash-attention
- **xFormers :** https://github.com/facebookresearch/xformers

---

## üöÄ Mode d'Emploi

1. **Installation automatique :** `installer_windows.bat`
2. **Diagnostic CUDA :** `diagnostic_cuda.py`
3. **Test g√©n√©ration :** `debug_quick.py`
4. **Interface compl√®te :** `main.py`

Le syst√®me d√©tecte automatiquement votre hardware et s√©lectionne le mod√®le optimal !
