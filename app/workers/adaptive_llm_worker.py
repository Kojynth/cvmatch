"""
Adaptive LLM Worker
==================

Worker adaptatif universel qui s'optimise automatiquement selon le GPU.
Garantie: G√©n√©ration sous 10 minutes sur TOUT syst√®me (GTX 1080 √† RTX 5070).
"""

import re
import time
import threading
from pathlib import Path
from typing import Dict, Any, Optional
from PySide6.QtCore import QThread, Signal, QTimer
from loguru import logger

# Imports adaptatifs selon disponibilit√©
OPTIMIZATIONS = {"vllm": False, "transformers": False, "torch": False}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
LINKEDIN_RE = re.compile(r"https?://[^\s]*linkedin\.com/[^\s]+", re.IGNORECASE)
PHONE_RE = re.compile(r"(?:\+?\d[\d\s().-]{8,}\d)")

try:
    import torch
    OPTIMIZATIONS["torch"] = True
except ImportError:
    pass

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    OPTIMIZATIONS["transformers"] = True
except ImportError:
    pass

try:
    import vllm
    from vllm import LLM, SamplingParams
    OPTIMIZATIONS["vllm"] = True
except ImportError:
    pass

from ..models.user_profile import UserProfile
from ..models.job_application import JobApplication, ApplicationStatus
from ..models.database import get_session
from ..utils.model_registry import model_registry
from ..utils.universal_gpu_adapter import universal_gpu_adapter
from ..utils.lightweight_model import lightweight_generator


class AdaptiveQwenManager:
    """Gestionnaire adaptatif qui choisit automatiquement la meilleure strat√©gie."""
    
    _instance = None
    _loaded_models = {}  # Cache des mod√®les charg√©s
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self.gpu_adapter = universal_gpu_adapter
        self.optimal_config = self.gpu_adapter.get_optimal_model_config()
        self.current_model_id = self.optimal_config.get("registry_key") or self._guess_profile_key(self.optimal_config.get("model_name"))
        self.performance_profile = self.gpu_adapter.performance_profile
        
        logger.info(f"üéØ Configuration adaptative: {self.performance_profile['tier']}")
        logger.info(f"‚è±Ô∏è Temps estim√©: {self.performance_profile['estimated_time_minutes']}min")
        
        self._initialized = True
    
    def _guess_profile_key(self, model_name: Optional[str]) -> Optional[str]:
        if not model_name:
            return None
        for profile in model_registry.list_profiles():
            if profile.model_id == model_name:
                return profile.key
        if isinstance(model_name, str) and '/' in model_name:
            return model_name.split('/')[-1]
        return model_name

    def get_optimal_model_config(self) -> Dict[str, Any]:
        """Retourne la configuration optimale (alias pour compatibilit√©)."""
        return self.optimal_config
    
    def load_model_adaptive(self, progress_callback=None):
        """Charge le mod√®le avec la strat√©gie optimale selon le GPU."""
        model_name = self.optimal_config["model_name"]
        
        # VERIFICATION AVANT TELECHARGEMENT
        if not self._is_model_locally_available(model_name):
            if progress_callback:
                progress_callback(f"‚ö†Ô∏è Mod√®le {model_name} non disponible localement")
                progress_callback("üö® T√©l√©chargement n√©cessaire - utilisation fallback")
            
            logger.warning(f"Mod√®le {model_name} n√©cessite t√©l√©chargement - fallback activ√©")
            raise RuntimeError(f"Mod√®le {model_name} non disponible localement")
        
        # V√©rifier si mod√®le d√©j√† en cache
        cache_key = f"{model_name}_{self.optimal_config['quantization']}"
        if cache_key in self._loaded_models:
            logger.info(f"üì¶ Mod√®le {model_name} trouv√© en cache")
            return self._loaded_models[cache_key]
        
        if progress_callback:
            progress_callback(f"üîß Chargement adaptatif {self.performance_profile['tier']}...")
        
        # Strat√©gie selon performance
        if self.optimal_config["use_vllm"] and OPTIMIZATIONS["vllm"]:
            model = self._load_with_vllm(progress_callback)
        elif OPTIMIZATIONS["transformers"]:
            model = self._load_with_transformers(progress_callback)
        else:
            raise RuntimeError("Aucun backend IA disponible - Installer transformers ou vllm")
        
        # Mettre en cache
        self._loaded_models[cache_key] = model
        
        if progress_callback:
            progress_callback(f"‚úÖ Mod√®le adaptatif charg√© - Mode {self.performance_profile['tier']}")
        
        return model
    
    def _load_with_vllm(self, progress_callback=None):
        """Chargement avec vLLM (ultra-rapide)."""
        if progress_callback:
            progress_callback("üöÄ Chargement vLLM ultra-rapide...")
        
        try:
            llm_config = {
                "model": self.optimal_config["model_name"],
                "tensor_parallel_size": 1,
                "gpu_memory_utilization": self.optimal_config["gpu_memory_utilization"],
                "max_model_len": self.optimal_config["max_new_tokens"],
                "trust_remote_code": True,
                "quantization": self.optimal_config["quantization"] if self.optimal_config["quantization"] != "fp16" else None
            }
            
            # Filtrer les param√®tres None
            llm_config = {k: v for k, v in llm_config.items() if v is not None}
            
            model = vllm.LLM(**llm_config)
            logger.info("üöÄ Mod√®le vLLM charg√© avec succ√®s")
            return {"engine": "vllm", "model": model}
            
        except Exception as e:
            logger.error(f"Erreur vLLM: {e}")
            # Fallback vers transformers
            if OPTIMIZATIONS["transformers"]:
                logger.info("üîÑ Fallback vers transformers...")
                return self._load_with_transformers(progress_callback)
            raise
    
    def _load_with_transformers(self, progress_callback=None):
        """Chargement avec Transformers (standard)."""
        if progress_callback:
            progress_callback("‚öôÔ∏è Chargement Transformers standard...")
        
        try:
            model_name = self.optimal_config["model_name"]
            device = self.optimal_config["device"]
            
            # Tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            # Configuration mod√®le selon quantification
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            
            # Quantification adaptative - CORRECTION: √©viter les conflits
            quantization = self.optimal_config["quantization"]
            if quantization == "gptq" and device == "cuda":
                # Utiliser seulement quantization_config, pas load_in_4bit
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif quantization == "awq" and device == "cuda":
                # Utiliser seulement quantization_config pour AWQ aussi
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
            elif quantization == "int8" and device == "cpu":
                # Pour CPU, pas de quantization_config
                logger.info("üíª Mode CPU - Quantification INT8 native")
            
            # Device map
            if device == "cuda":
                model_kwargs["device_map"] = "auto"
            
            # Chargement mod√®le
            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
            
            if device == "cpu":
                model = model.to("cpu")
            
            model.eval()
            
            logger.info(f"‚öôÔ∏è Mod√®le Transformers charg√© - {quantization}")
            return {
                "engine": "transformers",
                "model": model,
                "tokenizer": tokenizer,
                "device": device
            }
            
        except Exception as e:
            logger.error(f"Erreur Transformers: {e}")
            raise
    
    def generate_cv_adaptive(self, prompt: str, progress_callback=None, timeout_minutes=None, profile=None, offer_data=None) -> str:
        """G√©n√©ration adaptative avec syst√®me intelligent."""
        
        # NOUVEAU: Priorit√© au g√©n√©rateur l√©ger si mod√®le non disponible
        model_name = self.optimal_config["model_name"]
        
        if not self._is_model_locally_available(model_name):
            if progress_callback:
                progress_callback("üöÄ Mod√®le lourd non disponible - G√©n√©ration rapide activ√©e")
            
            logger.info("üöÄ Utilisation g√©n√©rateur l√©ger - Pas de t√©l√©chargement")
            
            if profile and offer_data:
                return lightweight_generator.generate_cv(
                    profile=profile,
                    offer_data=offer_data,
                    template="modern",
                    progress_callback=progress_callback
                )
            else:
                # Fallback classique si pas de donn√©es
                return self._generate_emergency_fallback(prompt, profile, offer_data)
        
        # G√©n√©ration IA classique si mod√®le disponible
        if timeout_minutes is None:
            timeout_minutes = self.optimal_config["timeout_minutes"]
        
        # Syst√®me de timeout strict
        result = {"cv": None, "error": None}
        
        def generation_task():
            try:
                model_data = self.load_model_adaptive(progress_callback)
                
                if model_data["engine"] == "vllm":
                    result["cv"] = self._generate_with_vllm(model_data, prompt, progress_callback)
                else:
                    result["cv"] = self._generate_with_transformers(model_data, prompt, progress_callback)
                    
            except Exception as e:
                result["error"] = str(e)
        
        # Lancer la g√©n√©ration dans un thread avec timeout
        thread = threading.Thread(target=generation_task)
        thread.daemon = True
        thread.start()
        
        # Attendre avec timeout
        thread.join(timeout=timeout_minutes * 60)
        
        if thread.is_alive():
            # Timeout d√©pass√©
            logger.error(f"‚è∞ Timeout {timeout_minutes}min d√©pass√© - Arr√™t forc√©")
            if progress_callback:
                progress_callback("‚è∞ Timeout - G√©n√©ration rapide de secours")
            
            # Utiliser g√©n√©rateur l√©ger en fallback
            if profile and offer_data:
                return lightweight_generator.generate_cv(profile, offer_data, "modern", progress_callback)
            else:
                return self._generate_emergency_fallback(prompt, profile, offer_data)
        
        if result["error"]:
            logger.error(f"Erreur g√©n√©ration: {result['error']}")
            # Utiliser g√©n√©rateur l√©ger en cas d'erreur
            if profile and offer_data:
                return lightweight_generator.generate_cv(profile, offer_data, "modern", progress_callback)
            else:
                return self._generate_emergency_fallback(prompt, profile, offer_data)
        
        return result["cv"] or self._generate_emergency_fallback(prompt, profile, offer_data)
    
    def _generate_with_vllm(self, model_data, prompt: str, progress_callback=None) -> str:
        """G√©n√©ration avec vLLM."""
        if progress_callback:
            progress_callback("üöÄ G√©n√©ration vLLM ultra-rapide...")
        
        formatted_prompt = self._format_prompt_for_vllm(prompt)
        
        sampling_params = vllm.SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=self.optimal_config["max_new_tokens"],
            repetition_penalty=1.1
        )
        
        outputs = model_data["model"].generate([formatted_prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        return self._clean_generated_text(generated_text)
    
    def _generate_with_transformers(self, model_data, prompt: str, progress_callback=None) -> str:
        """G√©n√©ration avec Transformers."""
        if progress_callback:
            progress_callback("‚öôÔ∏è G√©n√©ration Transformers...")
        
        model = model_data["model"]
        tokenizer = model_data["tokenizer"]
        device = model_data["device"]
        
        formatted_prompt = self._format_prompt_for_transformers(prompt)
        
        # Tokenisation
        inputs = tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(device)
        
        # G√©n√©ration
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=self.optimal_config["max_new_tokens"],
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # D√©codage
        generated_text = tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return self._clean_generated_text(generated_text)
    
    def _format_prompt_for_vllm(self, prompt: str) -> str:
        """Format prompt pour vLLM."""
        system_msg = "Tu es un expert en CV professionnels. Cr√©e un CV markdown adapt√© √† l'offre d'emploi."
        return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    def _format_prompt_for_transformers(self, prompt: str) -> str:
        """Format prompt pour Transformers."""
        return f"Cr√©er un CV professionnel en markdown pour cette offre d'emploi:\n\n{prompt}\n\nCV:"
    
    def _clean_generated_text(self, text: str) -> str:
        """Nettoie le texte g√©n√©r√©."""
        # Supprimer les balises de fin
        if "<|im_end|>" in text:
            text = text.split("<|im_end|>")[0]
        
        # Nettoyage basique
        text = text.strip()
        
        # S'assurer qu'on a un CV minimum
        if len(text) < 100:
            logger.warning("Texte g√©n√©r√© trop court - Ajout contenu minimal")
            text += "\n\n## Profil\nProfessionnel exp√©riment√© recherchant de nouveaux d√©fis.\n\n## Comp√©tences\n- Adaptabilit√©\n- Rigueur\n- Esprit d'√©quipe"
        
        return text
    
    def _is_model_locally_available(self, model_name: str) -> bool:
        """V√©rifie si un mod√®le est disponible localement sans t√©l√©chargement."""
        try:
            from transformers import AutoConfig
            from pathlib import Path
            import os
            
            # V√©rifier cache HuggingFace
            cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
            model_cache_name = model_name.replace("/", "--")
            model_cache_dir = cache_dir / f"models--{model_cache_name}"
            
            if model_cache_dir.exists():
                # V√©rifier si t√©l√©chargement complet
                blobs_dir = model_cache_dir / "blobs"
                if blobs_dir.exists():
                    incomplete_files = list(blobs_dir.glob("*.incomplete"))
                    if not incomplete_files:
                        logger.info(f"‚úÖ Mod√®le {model_name} disponible en cache")
                        return True
                    else:
                        logger.warning(f"‚è≥ Mod√®le {model_name} partiellement t√©l√©charg√© ({len(incomplete_files)} fichiers incomplets)")
                        return False
            
            # V√©rifier mod√®le local/custom
            if os.path.exists(model_name):
                return True
                
            logger.warning(f"‚ùå Mod√®le {model_name} non disponible localement")
            return False
            
        except Exception as e:
            logger.error(f"Erreur v√©rification mod√®le: {e}")
            return False
    
    def _generate_emergency_fallback(self, prompt: str, profile=None, offer_data=None) -> str:
        """G√©n√®re un CV de fallback avec les vraies donn√©es utilisateur."""
        logger.warning("üö® G√âN√âRATION FALLBACK D'URGENCE - Utilisation des donn√©es utilisateur")
        
        # Utiliser les param√®tres pass√©s ou des valeurs par d√©faut
        if profile:
            name = profile.name or "Nom non renseign√©"
            email = profile.email or "email@arenseigner.com"
            phone = profile.phone or "T√©l√©phone √† renseigner"
            linkedin = profile.linkedin_url or "LinkedIn √† renseigner"
            cv_content = profile.master_cv_content or ""
        else:
            name = "Candidat Professionnel"
            email = "candidat@email.com"
            phone = "T√©l√©phone √† renseigner"
            linkedin = "LinkedIn √† renseigner"
            cv_content = ""
        
        # Extraire infos de l'offre
        if offer_data:
            job_title = offer_data.get('job_title', 'Poste recherch√©')
            company = offer_data.get('company', 'Entreprise cible')
            offer_text = offer_data.get('text', '')[:300]
        else:
            job_title = 'Poste recherch√©'
            company = 'Entreprise cible'
            offer_text = ''
        
        # Extraire quelques √©l√©ments du CV ma√Ætre
        experience_section = ""
        skills_section = ""
        
        if cv_content:
            # Extraction basique d'exp√©rience et comp√©tences
            cv_lines = cv_content.split('\n')
            for i, line in enumerate(cv_lines):
                if any(keyword in line.lower() for keyword in ['exp√©rience', 'experience', 'emploi']):
                    # Prendre quelques lignes apr√®s
                    experience_section = '\n'.join(cv_lines[i:i+5])
                    break
            
            for i, line in enumerate(cv_lines):
                if any(keyword in line.lower() for keyword in ['comp√©tence', 'competence', 'skill']):
                    skills_section = '\n'.join(cv_lines[i:i+4])
                    break
        
        # Ajouter un marqueur FALLBACK visible
        fallback_cv = f"""# ‚ö†Ô∏è CV G√âN√âR√â EN MODE FALLBACK ‚ö†Ô∏è
# {name}

> **ATTENTION** : Ce CV a √©t√© g√©n√©r√© en mode fallback d'urgence.
> La g√©n√©ration IA compl√®te a √©chou√© - v√©rifiez votre configuration GPU/CUDA.

## Informations de contact
- **Email:** {email}
- **T√©l√©phone:** {phone}
- **LinkedIn:** {linkedin}

## Objectif professionnel
Candidature motiv√©e pour le poste de **{job_title}** chez **{company}**.

{f"**Contexte de l'offre :** {offer_text[:200]}..." if offer_text else ""}

## Profil professionnel
{cv_content[:400] + "..." if cv_content else "Professionnel exp√©riment√© recherchant de nouveaux d√©fis dans le domaine."}

## Exp√©rience professionnelle
{experience_section if experience_section else '''
### Exp√©rience r√©cente
**√Ä compl√©ter** | P√©riode
- Exp√©rience √† d√©tailler depuis votre CV ma√Ætre
- Responsabilit√©s principales
- R√©alisations mesurables
'''}

## Comp√©tences cl√©s
{skills_section if skills_section else '''
- Comp√©tences techniques √† reprendre de votre profil
- Ma√Ætrise des outils professionnels
- Capacit√©s d'adaptation et d'apprentissage
- Communication et travail en √©quipe
'''}

---
*CV g√©n√©r√© automatiquement en mode fallback - Merci de v√©rifier et compl√©ter les informations.*"""

        return fallback_cv
    
    def get_system_status(self) -> Dict[str, Any]:
        """Retourne le statut du syst√®me adaptatif."""
        guarantee_check = self.gpu_adapter.check_10_minute_guarantee()
        
        return {
            "gpu_info": self.gpu_adapter.gpu_info,
            "performance_tier": self.performance_profile["tier"],
            "model_selected": self.optimal_config["model_name"],
            "estimated_time_minutes": self.performance_profile["estimated_time_minutes"],
            "max_timeout_minutes": self.optimal_config["timeout_minutes"],
            "ten_minute_guarantee": guarantee_check["guarantee_met"],
            "optimizations_available": OPTIMIZATIONS,
            "recommendations": self.gpu_adapter.get_performance_recommendations()
        }


class AdaptiveCVGenerationWorker(QThread):
    """Worker adaptatif avec garantie 10 minutes."""
    
    progress_updated = Signal(str)
    generation_finished = Signal(dict)
    error_occurred = Signal(str)
    
    def __init__(self, profile: UserProfile, offer_data: dict, template: str):
        super().__init__()
        self.profile = profile
        self.offer_data = offer_data
        self.template = template
        self.adaptive_manager = AdaptiveQwenManager()
    
    def run(self):
        """Lance la g√©n√©ration adaptative avec timeout strict."""
        try:
            def progress_callback(message):
                self.progress_updated.emit(message)
            
            # Afficher le statut syst√®me
            status = self.adaptive_manager.get_system_status()
            progress_callback(f"üéØ Mode {status['performance_tier']} - Temps estim√©: {status['estimated_time_minutes']}min")
            
            if not status["ten_minute_guarantee"]:
                progress_callback("‚ö†Ô∏è Configuration pourrait d√©passer 10min - Optimisations appliqu√©es")
            
            # Construction prompt
            progress_callback("üìù Pr√©paration du prompt adaptatif...")
            prompt = self.build_adaptive_prompt()
            
            # G√©n√©ration avec timeout strict
            start_time = time.time()
            progress_callback("üöÄ G√©n√©ration IA adaptative...")
            
            # Timeout adaptatif selon le type de mod√®le
            model_config = self.adaptive_manager.get_optimal_model_config()
            is_cpu_model = model_config.get("device", "cpu") == "cpu"
            timeout_minutes = 20 if is_cpu_model else 10  # Plus de temps pour CPU
            
            progress_callback(f"‚è∞ Timeout configur√©: {timeout_minutes}min ({'CPU' if is_cpu_model else 'GPU'})")
            
            cv_markdown = self.generate_cv_with_fallback(
                prompt, 
                progress_callback,
                timeout_minutes=timeout_minutes
            )
            
            generation_time = (time.time() - start_time) / 60
            
            # Formatage
            try:
                cv_markdown = cv_markdown.format(
                    name=self.profile.name or "[Votre Prenom] [Votre Nom]",
                    email=self.profile.email or "[Votre Email]",
                    phone=self.profile.phone or "[Votre Telephone]",
                    linkedin=self.profile.linkedin_url or "[Votre LinkedIn]"
                )
            except KeyError:
                pass  # Ignorer les erreurs de formatage

            cv_markdown = self._force_profile_identity(cv_markdown)
            
            # Sauvegarde
            progress_callback("üíæ Sauvegarde...")
            application = self.save_application(cv_markdown, "")
            
            # R√©sultat
            result = {
                "application_id": application.id,
                "cv_markdown": cv_markdown,
                "cover_letter": "",
                "template": self.template,
                "model_version": status["model_selected"],
                "generation_time_minutes": round(generation_time, 2),
                "performance_tier": status["performance_tier"],
                "ten_minute_guarantee_met": generation_time <= 10
            }
            
            progress_callback(f"‚úÖ G√©n√©ration termin√©e en {generation_time:.1f}min")
            self.generation_finished.emit(result)
            
        except Exception as e:
            logger.error(f"Erreur worker adaptatif: {e}")
            self.error_occurred.emit(f"Erreur g√©n√©ration adaptative: {str(e)}")
    
    def build_adaptive_prompt(self) -> str:
        """Construit un prompt adaptatif optimis√©."""
        # Prompt simplifi√© pour les configurations faibles
        performance_tier = self.adaptive_manager.performance_profile["tier"]
        
        if performance_tier in ["cpu_fallback", "basic_performance"]:
            # Prompt court pour GPU faibles
            return f"""Cr√©er un CV pour:
Nom: {self.profile.name}
Email: {self.profile.email}
Poste: {self.offer_data['job_title']}
Entreprise: {self.offer_data['company']}

Profil: {self.profile.master_cv_content[:500] if self.profile.master_cv_content else 'Professionnel exp√©riment√©'}

Offre: {self.offer_data['text'][:800]}

Regles:
- N'invente pas de faits, utilise uniquement les donnees du profil.
- Adapte le CV a l'offre (mots-cles si presents dans le profil).
- Utilise les placeholders d'identite: [Votre Prenom] [Votre Nom], [Votre Email], [Votre Telephone], [Votre LinkedIn].

CV markdown professionnel et concis."""
        else:
            # Prompt complet pour GPU performants
            return f"""MISSION: Cr√©er un CV professionnel optimis√©.

CANDIDAT:
Nom: {self.profile.name}
Email: {self.profile.email}
T√©l√©phone: {self.profile.phone or 'Non renseign√©'}
LinkedIn: {self.profile.linkedin_url or 'Non renseign√©'}

CV de r√©f√©rence:
{self.profile.master_cv_content or 'Aucun CV de r√©f√©rence'}

OFFRE CIBLEE:
Poste: {self.offer_data['job_title']}
Entreprise: {self.offer_data['company']}
Description: {self.offer_data['text']}

OBJECTIFS:
1. CV sp√©cifiquement adapt√© √† cette offre
2. Mots-cl√©s pertinents int√©gr√©s
3. Structure markdown professionnelle
4. Template: {self.template}

REGLES:
- Ne jamais inventer de faits.
- Utiliser les mots-cles de l'offre uniquement s'ils existent dans les donnees candidat.
- Utiliser les placeholders d'identite: [Votre Prenom] [Votre Nom], [Votre Email], [Votre Telephone], [Votre LinkedIn].

Cr√©er un CV professionnel en markdown."""

    def _force_profile_identity(self, cv_markdown: str) -> str:
        """Force the profile identity in the generated markdown."""
        if not cv_markdown:
            return cv_markdown

        lines = cv_markdown.splitlines()
        name = (self.profile.name or "[Votre Prenom] [Votre Nom]").strip()
        email = (self.profile.email or "[Votre Email]").strip()
        phone = (self.profile.phone or "[Votre Telephone]").strip()
        linkedin = (self.profile.linkedin_url or "[Votre LinkedIn]").strip()

        if name:
            replaced = False
            for idx, line in enumerate(lines):
                if line.strip().startswith("# "):
                    lines[idx] = f"# {name}"
                    replaced = True
                    break
            if not replaced:
                lines.insert(0, f"# {name}")

        if email:
            for idx, line in enumerate(lines):
                if "@" in line or "email" in line.lower():
                    updated = EMAIL_RE.sub(email, line)
                    if updated == line and "email" in line.lower():
                        updated = f"- Email: {email}"
                    lines[idx] = updated

        if phone:
            for idx, line in enumerate(lines):
                lowered = line.lower()
                if any(token in lowered for token in ["tel", "telephone", "phone", "mobile"]):
                    updated = PHONE_RE.sub(phone, line)
                    if updated == line:
                        updated = f"- Telephone: {phone}"
                    lines[idx] = updated

        if linkedin:
            for idx, line in enumerate(lines):
                if "linkedin" in line.lower():
                    updated = LINKEDIN_RE.sub(linkedin, line)
                    if updated == line:
                        updated = f"- LinkedIn: {linkedin}"
                    lines[idx] = updated

        return "\n".join(lines).strip()
    
    def save_application(self, cv_markdown: str, cover_letter: str) -> JobApplication:
        """Sauvegarde rapide."""
        application = JobApplication(
            profile_id=self.profile.id,
            job_title=self.offer_data['job_title'],
            company=self.offer_data['company'],
            offer_text=self.offer_data['text'][:1000],  # Limiter pour performance
            template_used=self.template,
            model_version_used="Adaptive",
            generated_cv_markdown=cv_markdown,
            generated_cover_letter=cover_letter,
            status=ApplicationStatus.DRAFT
        )
        
        with get_session() as session:
            session.add(application)
            session.commit()
            session.refresh(application)
        
        return application
    
    def generate_cv_with_fallback(self, prompt: str, progress_callback=None, timeout_minutes=None) -> str:
        """G√©n√©ration avec fallback intelligent utilisant les donn√©es du worker."""
        try:
            # Essayer la g√©n√©ration adaptative avec donn√©es du worker
            cv_result = self.adaptive_manager.generate_cv_adaptive(
                prompt, 
                progress_callback,
                timeout_minutes=timeout_minutes,
                profile=self.profile,  # Passer le profil
                offer_data=self.offer_data  # Passer les donn√©es d'offre
            )
            return cv_result
        except Exception as e:
            # En cas d'erreur, utiliser le fallback avec les vraies donn√©es
            logger.warning(f"G√©n√©ration √©chou√©e, utilisation fallback: {e}")
            return self.adaptive_manager._generate_emergency_fallback(
                prompt, 
                profile=self.profile, 
                offer_data=self.offer_data
            )


# Instance globale
adaptive_qwen_manager = AdaptiveQwenManager()
